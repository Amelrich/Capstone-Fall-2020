{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "kassie_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amelrich/Capstone-Fall-2020/blob/kassie-preprocessing/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faTOmY4IQ3BO"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_datareader.data as web\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwkiiSn4vQ0u"
      },
      "source": [
        "!pip install pandas_market_calendars"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUlVSZ14GiUU"
      },
      "source": [
        "import pandas_market_calendars as mcal"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJQjD0txQ3BY"
      },
      "source": [
        "# Start and End date of stock data\n",
        "start_date = pd.to_datetime('1999-11-18')\n",
        "end_date   = pd.to_datetime('2020-09-02')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t7bJ0BASzzJ"
      },
      "source": [
        "# Read names of Stocks we are interested in\n",
        "symbols = pd.read_csv('https://raw.githubusercontent.com/Amelrich/Capstone-Fall-2020/kassie-preprocessing/sp500.csv',index_col=False)\n",
        "symbols = list(symbols['Symbol'].values)\n",
        "symbols = sorted(symbols)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SLbplXPvFFh"
      },
      "source": [
        "symbols = ['BF-B' if x=='BF.B' else x for x in symbols]\n",
        "symbols = ['BRK-B' if x=='BRK.B' else x for x in symbols]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyIiR8qPthRA"
      },
      "source": [
        "###Set start & end date and find the trading days between them\n",
        "From the `pandas_market_calendars` package, find all the trading dates given a specific range of dates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PxgHLXomkeB"
      },
      "source": [
        "# get trading days calendar\n",
        "def create_market_cal(start, end):\n",
        "    nyse = mcal.get_calendar('NYSE')\n",
        "    schedule = nyse.schedule(start, end)\n",
        "    market_cal = mcal.date_range(schedule, frequency='1D')\n",
        "    market_cal = market_cal.tz_localize(None)\n",
        "    market_cal = [i.replace(hour=0) for i in market_cal]\n",
        "    return market_cal \n",
        "\n",
        "\n",
        "# Start and End date of stock data\n",
        "start_date = pd.to_datetime('1999-11-18')\n",
        "end_date   = pd.to_datetime('2020-09-02')\n",
        "\n",
        "# Create a calendar\n",
        "calendar = create_market_cal(start_date, end_date)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3K1_8nMIOp2"
      },
      "source": [
        "### Split the calendar dates in chunks of length 150\n",
        "We do this because we are interested in data points that capture multiple time scales. We want to capture a stocks behavior over 50 & 100 & 150 days, and have all of this information in one data point that has a fixed length of 50."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmqG9fLpILNG",
        "outputId": "07efd3a1-1cc6-4884-ae89-d8a61e8cfff1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# divide the calendar in 150 day chunks\n",
        "def divide_chunks(l, n): \n",
        "      \n",
        "    # looping till length l \n",
        "    for i in range(0, len(l), n):  \n",
        "        yield l[i:i + n] \n",
        "\n",
        "# split trading days from start to end date in 50 day chunks\n",
        "n = 150\n",
        "list_of_date_chunk = list(divide_chunks(calendar, n)) \n",
        "\n",
        "# we skip the last date chunk bc it is not 150 days long\n",
        "list_of_date_chunk = list_of_date_chunk[:-1]\n",
        "len(list_of_date_chunk[-1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVozYUe0JK-Z"
      },
      "source": [
        "### How we achieve multiscale data points\n",
        "\n",
        "Two methods\n",
        "\n",
        "* By picking prices every consecutive, alterate and every 2 days \n",
        "\n",
        "* By picking consecutive prices for 50, 100, 150 days.\n",
        "\n",
        "Please see methods `get_multiscale_skipped_values` and `get_multiscale_consecutive_values`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FjOP1kBRwxw",
        "outputId": "e5348f07-46b8-49b1-c7f7-ba4f5b33e138",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def get_multiscale_skipped_values(df_start_end):\n",
        "  days_50 = df_start_end[-50:]\n",
        "  days_100 = df_start_end[-100::2]\n",
        "  days_150 = df_start_end[-150::3]\n",
        "  dictionary = {'50_days': days_50['Adj Close'].values, '100_days': days_100['Adj Close'].values,\n",
        "        '150_days': days_150['Adj Close'].values}\n",
        "  return dictionary\n",
        "\n",
        "def get_multiscale_consecutive_values(df_start_end):\n",
        "  days_50 = df_start_end[-50:]\n",
        "  days_100 = df_start_end[-100:]\n",
        "  days_150 = df_start_end[-150:]\n",
        "\n",
        "  days_50_all = pd.DataFrame(np.append(np.zeros((100,3)), days_50 ,0))\n",
        "  days_50_all.loc[:, 0] = pd.to_numeric(days_50_all.loc[:, 0])\n",
        "  days_50_all.columns = days_50.columns\n",
        "  days_100_all = pd.DataFrame(np.append(np.zeros((50,3)), days_100,0))\n",
        "  days_100_all.loc[:, 0] = pd.to_numeric(days_100_all.loc[:, 0])\n",
        "  days_100_all.columns = days_50.columns\n",
        "  dictionary = {'50_days': days_50_all['Adj Close'].values, '100_days': days_100_all['Adj Close'].values,\n",
        "                    '150_days': days_150['Adj Close'].values}\n",
        "  return dictionary\n",
        "\n",
        "def scrape_yahoo(stock_name, start_date, end_date):\n",
        "  # scrape data of each stock from yahoo\n",
        "  try:\n",
        "    df = web.DataReader(stock_name,'yahoo', start_date, end_date)\n",
        "    df = df[['Adj Close','Volume']]\n",
        "    df['Symbol'] =  stock_name\n",
        "    find_flag = 1\n",
        "    return df, find_flag\n",
        "  except KeyError:\n",
        "    print(\"Could not find data on \".format(stock_name))\n",
        "    find_flag = 0\n",
        "    return pd.DataFrame(), find_flag\n",
        "\n",
        "total_prices_list_skipped_values = []\n",
        "total_labels_list_consecutive_values = []\n",
        "total_labels_list = []\n",
        "n = 0\n",
        "for stock_name in symbols:\n",
        "  n = n + 1\n",
        "  if n%100 == 0:\n",
        "    print(\"{} stocksout of {} completed\".format(n,len(symbols)))\n",
        "  stock_df, find_flag = scrape_yahoo(stock_name, start_date, end_date)\n",
        "  \n",
        "  if find_flag == 0:\n",
        "    print(\"Could not find data on {}\".format(stock_name))\n",
        "    continue\n",
        "\n",
        "  prices_list_skipped_values = []\n",
        "  prices_list_consecutive_values = []\n",
        "  stock_name_list_values = []\n",
        "\n",
        "  for item in list_of_date_chunk:\n",
        "    start = item[0]\n",
        "    end = item[-1]\n",
        "    \n",
        "    df_start_end = stock_df.loc[start:end]\n",
        "    if len(df_start_end) >= 150:\n",
        "      dictionary_skipped_values = get_multiscale_skipped_values(df_start_end)\n",
        "      dictionary_consecutive_values = get_multiscale_consecutive_values(df_start_end)\n",
        "    else:\n",
        "      continue\n",
        "    \n",
        "    datapoint_skipped_values = pd.DataFrame(dictionary_skipped_values).to_numpy()\n",
        "    datapoint_consecutive_values = pd.DataFrame(dictionary_consecutive_values).to_numpy()\n",
        "\n",
        "    prices_list_skipped_values.append(datapoint_skipped_values)\n",
        "    prices_list_consecutive_values.append(datapoint_consecutive_values)\n",
        "    stock_name_list_values.append(stock_name)\n",
        "  \n",
        "  total_prices_list_skipped_values.append(prices_list_skipped_values)\n",
        "  total_labels_list_consecutive_values.append(prices_list_consecutive_values)\n",
        "  total_labels_list.append(stock_name_list_values)\n",
        "  #print('Stock {} is done'.format(stock_name))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 stocksout of 505 completed\n",
            "200 stocksout of 505 completed\n",
            "300 stocksout of 505 completed\n",
            "400 stocksout of 505 completed\n",
            "500 stocksout of 505 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9ZiySh6F8-4"
      },
      "source": [
        "# we flatten the above total lists that contain all prices for all stocks in S&P\n",
        "flattened_list_skipped_values = [y for x in total_prices_list_skipped_values for y in x]\n",
        "flattened_list_consecutive_values = [y for x in total_labels_list_consecutive_values for y in x]\n",
        "flattened_list_stock_names = [y for x in total_labels_list for y in x]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB8U0Sq6NTz2"
      },
      "source": [
        "np.save(\"/content/drive/My Drive/capstone/skipped_values.npy\", flattened_list_skipped_values)\n",
        "np.save(\"/content/drive/My Drive/capstone/consecutive_values.npy\", flattened_list_consecutive_values)\n",
        "np.save(\"/content/drive/My Drive/capstone/stock_names.npy\", flattened_list_stock_names)"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}