{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoders_preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCbskjK4vTki"
      },
      "source": [
        "# Autoencoders\n",
        "\n",
        "For Colab, it downloads the data using the following code. \n",
        "\n",
        "If you work locally be sure to have the folder *data/*. You can download it from the repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e6_qZp3jg8t"
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "  !wget -q https://github.com/Amelrich/Capstone-Fall-2020/archive/master.zip\n",
        "  !unzip -q master.zip\n",
        "  !rm -rf data\n",
        "  !mv Capstone-Fall-2020-master/data/ data/\n",
        "  !mv Capstone-Fall-2020-master/TS_utils.py /content/\n",
        "  !rm -rf master.zip Capstone-Fall-2020-master/\n",
        "except:\n",
        "  print(\"only in Colab\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPGTRcWuKqw-"
      },
      "source": [
        "import numpy as np\n",
        "import numpy.random as rd\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "from TS_utils import TS_generator, Synthetic_TS_generator\n",
        "from TS_utils import MedianScaler, DCT_lowpass_filter\n",
        "from TS_utils import KMedians\n",
        "\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxynbyjeHimO"
      },
      "source": [
        "# **Autoencoders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC0_wT46iKLx"
      },
      "source": [
        "import keras\n",
        "from keras import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4F7d3wwmFQM"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMGxjVHGHIwb"
      },
      "source": [
        "class Autoencoder:\n",
        "    def __init__(self, time_step, embedding_size,\n",
        "                 n_features=1, plot_model=False):\n",
        "      \n",
        "        self.encoder = Sequential()\n",
        "        self.encoder.add(Dense(embedding_size, input_shape=(time_step,)))\n",
        "\n",
        "        self.decoder = Sequential()\n",
        "        self.decoder.add(Dense(time_step))\n",
        "\n",
        "        self.autoencoder = Model( self.encoder.input, self.decoder(self.encoder.output) )\n",
        "        self.encoding_model = Model( self.encoder.input, self.encoder.output )\n",
        "\n",
        "        opt = Adam(learning_rate=0.005)\n",
        "        self.autoencoder.compile(optimizer=opt, loss='mse')\n",
        "\n",
        "        if plot_model:\n",
        "          print(self.autoencoder.summary())\n",
        "\n",
        "    def fit(self, X_train, X_test, epochs=25, verbose=0):\n",
        "        self.autoencoder.fit(X_train, X_train, batch_size=16, epochs=epochs, shuffle=True,\n",
        "                             verbose=verbose, validation_data=(X_test, X_test))\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoding_model.predict(x)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.autoencoder.predict(x)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi8waZo-KaJa"
      },
      "source": [
        "def AE_clustering(X, n_short=None, n_clusters=7):\n",
        "\n",
        "  print('#'*54)\n",
        "  print('##### Welcome to Autoencoder clustering pipeline ##### ')\n",
        "  print('#'*54, '\\n')\n",
        "\n",
        "  long_scale = X.shape[1]\n",
        "  if n_short == None:\n",
        "    short_scale = long_scale // 3\n",
        "  else:\n",
        "    short_scale = n_short\n",
        "\n",
        "  print('Scaling raw data')\n",
        "  lowpass_filter = DCT_lowpass_filter()\n",
        "  scaler = MinMaxScaler()\n",
        "\n",
        "  Xscale = lowpass_filter.fit_transform(X)\n",
        "  Xscale = scaler.fit_transform(Xscale.T).T\n",
        "\n",
        "  X_train, X_val = Xscale[:X.shape[0]//2,:], Xscale[X.shape[0]//2:,:]\n",
        "\n",
        "  autoencoder = Autoencoder(time_step=long_scale, embedding_size=short_scale)\n",
        "  print('Training the autoencoder neural network...')\n",
        "  autoencoder.fit(X_train, X_val, epochs=100, verbose=0)\n",
        "  print('Training process done!')\n",
        "  del(X_train)\n",
        "  del(X_val)\n",
        "\n",
        "  print('Clustering...')\n",
        "  embedding = autoencoder.encode(Xscale)\n",
        "  embedding = MinMaxScaler().fit_transform(embedding.T).T\n",
        "  X_cluster = np.concatenate([Xscale[:,-short_scale:], embedding], axis=1)\n",
        "\n",
        "  cl = KMeans(n_clusters=n_clusters)\n",
        "  y_train_pred = cl.fit_predict(X_cluster)\n",
        "  print('Clustering done! Your turn Hritik ;)')\n",
        "  del(Xscale)\n",
        "\n",
        "  return y_train_pred"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWMvTAwZUCcb"
      },
      "source": [
        "## Exemple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8TCGH7jUo9p"
      },
      "source": [
        "short scale recommended from long scale:\n",
        "* 80 -> 30\n",
        "* 70 -> 25\n",
        "* 60 -> 20\n",
        "* 50 -> 20\n",
        "* 40 -> 15\n",
        "* 30 -> 10\n",
        "* 20 -> 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uP8UKykQzsK",
        "outputId": "a5fa8084-672f-48ba-bc3d-c7de0dc41c17"
      },
      "source": [
        "gen = TS_generator(nb_timeseries=2000, chunk_size=60)\n",
        "X = gen.get_array()\n",
        "\n",
        "AE_clustering(X, n_short=20, n_clusters=7)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "######################################################\n",
            "##### Welcome to Autoencoder clustering pipeline ##### \n",
            "###################################################### \n",
            "\n",
            "Scaling raw data\n",
            "Training the autoencoder neural network...\n",
            "Training process done!\n",
            "Clustering...\n",
            "Clustering done! Your turn Hritik ;)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 0, 5, ..., 5, 0, 1], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYZZ9EdXUBDL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wU1D4lIWIYU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}